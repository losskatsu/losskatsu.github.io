---
title: "[선형대수] 고유값(eigenvalue) 고유벡터(eigenvector)의 의미" 
categories:
  - linear-algebra
tags:
  - statistics
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---


# 고유값(eigenvalue) 고유벡터(eigenvector)


### 참고링크  

|머신러닝|딥러닝|선형대수|기초통계|최적화|
|:------:|:------:|:------:|:------:|:------:|
|[k-means](https://losskatsu.github.io/machine-learning/kmeans-clustering/)|[신경망이란](https://losskatsu.github.io/machine-learning/dl-basic01/)|[고유값,고유벡터](https://losskatsu.github.io/linear-algebra/eigen/)| [확률변수](https://losskatsu.github.io/statistics/random-variable/) |[컨벡스 셋](https://losskatsu.github.io/machine-learning/convex-set/)|
|[k-최근접이웃](https://losskatsu.github.io/machine-learning/knn/)|[성능함수](https://losskatsu.github.io/machine-learning/dl-basic02/)|[행렬식](https://losskatsu.github.io/linear-algebra/determinant/)| [확률분포](https://losskatsu.github.io/statistics/prob-distribution/) | [컨벡스 함수](https://losskatsu.github.io/machine-learning/convex-function/)|
|[선형회귀](https://losskatsu.github.io/statistics/simple-regression/)|[신경망 학습](https://losskatsu.github.io/machine-learning/dl-basic03/)|[내적](https://losskatsu.github.io/linear-algebra/innerproduct/)| [모집단과 표본](https://losskatsu.github.io/statistics/population-sample/) |[라그랑주 듀얼](https://losskatsu.github.io/machine-learning/dual-function/)|
|[로지스틱회귀](https://losskatsu.github.io/statistics/logistic-regression/) |[교차연결](https://losskatsu.github.io/machine-learning/dl-basic04/) |[기저](https://losskatsu.github.io/linear-algebra/basis/)| [평균과 분산](https://losskatsu.github.io/statistics/mean-vairance/)   | [KKT 조건](https://losskatsu.github.io/machine-learning/kkt/) |
|[릿지,라쏘회귀](https://losskatsu.github.io/machine-learning/l1l2/) |[합성곱 신경망](https://losskatsu.github.io/machine-learning/dl-basic05/) |[랭크, 차원](https://losskatsu.github.io/linear-algebra/rank-dim/)| [공분산, 상관계수](https://losskatsu.github.io/statistics/cov-corr/)  | [ROC 커브](https://losskatsu.github.io/machine-learning/stat-roc-curve/) |
|[의사결정나무](https://losskatsu.github.io/machine-learning/decision-tree/) |[배치, 에포크 차이](https://losskatsu.github.io/machine-learning/epoch-batch/) | [선형변환](https://losskatsu.github.io/linear-algebra/linear-trans/)| [최대가능도추정](https://losskatsu.github.io/statistics/mle/) | [크로스 밸리데이션](https://losskatsu.github.io/machine-learning/cross-validation/) |
|[서포트벡터머신](https://losskatsu.github.io/machine-learning/svm/) | [텐서플로기초(1)](https://losskatsu.github.io/machine-learning/tensorflow-basic01/) |[직교행렬](https://losskatsu.github.io/linear-algebra/orthogonal/) | [베르누이,이항분포](https://losskatsu.github.io/statistics/binomial/)  | [실루엣 스코어](https://losskatsu.github.io/machine-learning/silhouette-score) |
|[원클래스 SVM](https://losskatsu.github.io/machine-learning/oneclass-svm/)  | [텐서플로기초(2)](https://losskatsu.github.io/machine-learning/tensorflow-basic02/)  | [고유값분해](https://losskatsu.github.io/linear-algebra/eigen-decomposition/)| [기하,음이항분포](https://losskatsu.github.io/statistics/geometric-negative/) | |
|[LDA ](https://losskatsu.github.io/machine-learning/lda/) | [seq2seq](https://losskatsu.github.io/machine-learning/seq2seq-keras/) | [특이값분해](https://losskatsu.github.io/linear-algebra/svd/) | [초기하분포](https://losskatsu.github.io/statistics/hypergeometric/) | |
|[GMM](https://losskatsu.github.io/machine-learning/gmm/) | [opencv기초](https://losskatsu.github.io/machine-learning/opencv01) | |[포아송분포](https://losskatsu.github.io/statistics/poisson/) | |
|[부스팅](https://losskatsu.github.io/machine-learning/boosting/) | [resnet](https://losskatsu.github.io/machine-learning/resnet) | | [정규분포](https://losskatsu.github.io/statistics/normaldist/) | |
|[사이킷런 실습](https://losskatsu.github.io/machine-learning/sklearn/) |[다각형내부판별](https://losskatsu.github.io/machine-learning/py-polygon01) | |[감마분포](https://losskatsu.github.io/statistics/gammadist/) | |
| | [엣지판별](https://losskatsu.github.io/machine-learning/edge-detect-canny) | | [지수분포](https://losskatsu.github.io/statistics/exponentialdist/) | |
| | | | [카이제곱분포](https://losskatsu.github.io/statistics/chisquareddist/) | |
| | | | [베타분포](https://losskatsu.github.io/statistics/betadist/) | |
| | | | [균일분포](https://losskatsu.github.io/statistics/uniformdist/) | |


<br/>

<a href="http://www.yes24.com/Product/Goods/97032765" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00001_ml.png" width="800" align="middle">

<br/>



저는 선형대수에서 가장 중요한 부분으로 고유값, 고유벡터를 꼽겠습니다. 
고유값, 고유벡터는 선형대수 전체를 아우르는 중요한 개념이라고 생각합니다. 
일반적인 선형대수 책에서 고유값, 고유벡터를 찾으면 다음과 같은 정의을 아주 쉽게 찾아 보실 수 있습니다.  

$ Ax = \lambda  x$ 

> $\lambda$가 실수 공간에 속할때($\lambda \in R$), 정방행렬(square matrix) $A$의 고유벡터는 영벡터가 아닌 벡터(nonzero vector) $x$이다. 
또한, $\lambda$는 $A$의 고유값이다.
<br />

이 정의가 의미하는 바는 무엇일까요? 아마 교과서 정의 만으로는 이해하기 어려울 수 있습니다. 
지금부터 저와함께 차근차근 그 의미를 알아봅시다.
<br />

먼저 정방행렬 $A$의 의미를 알아봅시다.

## 정방행렬 A의 의미
우리가 알고 있는 행렬에는 여러가지 의미가 포함되어 있지만, 
고유값, 고유벡터를 논할 때, 정방행렬은 선형변환을 의미합니다. 
그렇다면 선형변환은 무엇일까요? 

## 선형변환의 의미
선형변환은 쉽게 말해서 좌표공간 내에서 일어날 수 있는 모든 변환이라고 생각하시면 편합니다. 
예를 들어, 좌표평면에 벡터 하나가 있다고 가정하면, 
그 벡터를 확대하거나, 축소하거나, 회전시키거나, 반사시키는 것은 모두 변환이라고 생각할 수 있습니다. 
물론 엄밀하게 말하면 이는 틀린 설명이라고 할수도 있으나, 처음 접하실때는 이렇게 이해해도 문제 없다고 생각합니다. 
아래 그림에서 원본 벡터 v는 다양한 선형변환을 통해 v1, v2, v3로 변환될 수 있습니다. 

![/assets/images/eigen/lineartransformation.JPG](/assets/images/eigen/lineartransformation.JPG)

## Ax의 의미
결국 Ax 는 x라는 벡터에 선형변환(A)을 취한 것을 의미합니다. 
벡터 x를 늘리거나 줄이거나 회전시키거나 하는 등 어떤 '변환'을 취한 것이지요.

## '고유', '벡터'의 의미 
벡터라는 단어는 많이 들어보셨을 거라 생각합니다. 
벡터를 구성하는 요소에는 두 가지가 있습니다. 
여러분도 잘 알다시피 '방향(direction)'과 '크기(magnitude)' 가 그것입니다. 

고유, 즉 eigen은 어떤 뜻일까요? 
eigen은 독일어 인데, "own", "peculiar to", "characteristic", "individual"이라는 뜻입니다.
여러분은 이 단어들 중 어떤 단어가 끌리시나요? 
저는 characteristic 이라는 단어가 끌립니다. 
만약 여러분이 책을 보다가 characteristic 이라는 단어가 나온다면 집중하실 필요가 있습니다. 
characteristic은 여러 분야에서 중요한 뜻으로 쓰이더라구요. 
characteristic은 무슨 뜻일까요? 
네이버 영어사전을 찾아보니 **"특유의"**, **"특징"** 이라는 뜻이 나오네요. 
따라서 고유벡터란 어떤 특징을 가지고 있는 벡터라는 것으로 추측할 수 있습니다. 
여기서 말하는 특징은 방향(direction)은 변하지 않고, 크기(magnitude)만 변하는 특징을 말합니다. 
즉, 고유벡터란 어떤 선형변환을 취했을 때, 방향은 변하지 않고 크기만 변하는 벡터를 의미 합니다. 

## 고유값의 의미
앞서 고유벡터의 크기가 변한다고 했는데, 얼마나 변할까요?
바로 그 변한 크기가 고유값을 뜻합니다.
만약 고유값이 2라면 기존벡터 크기의 2배만큼 길어진 것이고, 
고유값이 1/3 이라면 기존 벡터 크기의 1/3 만큼 줄어든 것이지요. 

## 고유값, 고유벡터의 의미 
마지막 정리입니다. 
고유벡터란 어떤 벡터에 선형변환을 취했을때, 방향은 변하지 않고 크기만 변환되는 벡터를 의미하고, 
고유값이란 고유벡터가 변환되는 '크기'를 의미합니다. 


<br/>

<a href="http://www.yes24.com/Product/Goods/105772247" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00002_la.png" width="800" align="middle">

<br/>
  
