---
title: "[딥러닝] Attention is all you need 논문 번역" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---


# Attention is all you need 논문 번역

## 0. Abstract

대부분의 시퀀스 변환 모델들은 복잡한 순환신경망(RNN)이나 합성곱신경망(CNN)을 기반으로 하는 경우가 많았고, 
이러한 신경망들은 인코더(encoder)와 디코더(decoder)를 포함한다. 
또한 성능이 가장 뛰어난 모델들은 어텐션 메커니즘을 통해 인코더와 디코더를 연결한다. 
우리는 이 논문에서 트랜스포머(Transformer)라는 아주 단순한 신경망 구조를 제안한다. 
이 트랜스포머라는 구조는 RNN이나 CNN은 제거하고 단순히 어텐션 메커니즘을 기반으로 하는 아주 간단한 구조이다. 
우리는 번역 업무를 수행하는 두 종류의 인공지능으로 실험을 했는데, 
트랜스포머 기반의 모델이 품질이 더 우수했으며 병렬처리 하기 좋고, 학습하는 시간도 적게들었다. 
우리 모델은 WMT 2014 영어-독일어 번역 업무에서 28.4 BLEU를 기록했는데, 
이는 기존 기록보다 2 BLEU 이상 좋은 성능을 보여주면서 기존 베스트 기록을 갈아치웠다. 
그리고 WMT 2014 영어-프랑스 번역 업무에서는 단일 모델 기준, 
41.8 BLEU라는 초대 신기록을 달성했는데, 
이는 3.5일 동안 8개의 GPU로 학습한 결과이고 흔히 말하는 베스트 모델에 비해 현저히 적은 리소스로 학습한 것을 의미한다. 
트랜스포머 구조는 대규모 학습 데이터냐 제한된 학습 데이터냐에 상관없이 
영어 구문 분석에서 성공적으로 적용할 수 있음을 보여주었다. 

## 1. Introduction

RNN, LSTM, GRU와 같은 모델들은 언어 모델이나 기계번역과 같은 시퀀스 모델링이나 변환 문제에서 대세로 자리잡았습니다. 
그리고 인코더-디코더 아키텍트를 발전시키기 위한 수많은 사람들의 노력이 있었다. 

순환 모델들은 일바적으로 인풋이나 아웃풋 시퀀스의 기호 위치를 따라 팩터를 계산한다. 
시퀀스의 기호 위치는 계산 과정에서 특정 스텝에 할당딘 후, 
은닉 상태 $h_t$를 생성합니다. 
이때 은닉 상태 $h_t$는 이전 은닉상태 $h_{t-1}$과 현재 시점 $t$에서의 인풋값의 함수이다. 
그러나 이러한 방식으로는 학습과정에서 데이터의 병렬 처리를 어렵게 만들고 시퀀스 길이가 길어질수록 더 큰 문제가 된다.  
왜냐하면 메모리 제약조건이 데이터간 배치를 제한하기 때문이다. 
최근 연구에서는 팩토라이제이션 트릭과 조건부 계산을 활용해 계산 효율성을 높이고, 
조건부 계산의 경우에는 모델 성능도 끌어올렸다. 
그러나 순차적으로 계산된다는 근본적인 제약조건은 여전히 남아있는 상태였다.  

어텐션은 인풋/아웃풋 시퀀스 내의 거리에 상관없이 의존성을 모델링 가능하게 만들어, 
다양한 작업에서 강력한 시퀀스 모델링 통합이나 변환 모델의 필수 요소가 되었다. 
그러나 일부 예외케이스를 제외하면 이러한 어텐션 메커니즘은 대부분 RNN과 함께 사용된다. 

이 연구에서는 RNN을 배제하고, 대신 어텐션 메커니즘만을 사용해 
인풋과 아웃풋 간의 의존성을 모델링하는 Transformer라는 모델 아키텍처를 제안한다. 
Transformer는 병렬 처리를 대폭 향상 시키며, 
8개의 P100 GPU에서 단 12시간의 훈련만으로 번역에서 최고의 성능을 보여준다. 

## 2. Background

순차적 계산을 줄이기 위한 목표는 Extended Neural GPU, ByteNet, ConvS2S와 같은 
모델들의 기반을 형성한다. 
이러한 모델들은 기본적으로 CNN을 사용하며 
모든 인풋/아웃풋 위치에 대해 은닉 표현(hidden representation)을 병렬처리로 계산한다. 
그리고 이러한 모델들은 엄청난 양의 연산을 요구한다. 
왜냐하면 임의의 두 인풋 또는 아웃풋 위치 간의 신호를 연결하는데 필요한 연산 횟수가 
위치간 거리에 따라 증가하기 때문이다. 
ConvS2S는 선형적으로, ByteNet의 경우 로그 함수 형태로 증가한다. 
이는 먼 위치간의 의존성을 학습하는 것을 더 어렵게 만든다. 
반면, 트랜스포머에서는 연산 횟수가 상수형태로 일정하다는 장점이 있지만, 
이를 위해 어텐션 가장치가 적용된 위치를 평균화하기 때문에 
모델이 정보를 구분하고 세밀하게 표현하는 능력이 감소한다는 단점이 있다. 
이러한 효과는 3.2절에서 설명한 Multi-Head Attention을 통해 보완한다. 

Self-attention은 때로 intra-attention이라고도 불리는데, 
이는 하나의 시퀀스 내의 서로 다른 위치들을 연관지어 
그 시퀀스의 표현(representation)을 계산하는 어텐션 메커니즘이다. 
Self-attention은 다양한 역할을 수행하는데, 
예를 들어, 독해, 요약, 텍스트 함의 분석 등과 같은 문장 표현 학습을 포함한 
다양한 역할을 수행한다. 

End-to-end 메모리 네트워크는 시퀀스 정렬된 순환 대신 
순환 어텐션 메커니즘을 기반으로 하며, 
간단한 언어 질문 답변 및 언어 모델링 관련해서는 좋은 성능을 보이는 것으로 알려져있다. 
여기서 시퀀스 정렬(sequence alignment)라는 말은 
입력 시퀀스의 각 위치가 시간 순서대로 단계별로 처리되는 것을 의미한다. 

우리가 아는 한 트랜스포머는 시퀀스 정렬된 RNN이나 합성곱 없이도 
인풋/아웃풋 표현을 계산하기 위해 self-attention만 사용하는 최초의 변환 모델이다. 
다음 섹션에서는 트랜스포머를 설명하고, self-attention을 사용하는 이유를 알아보며, 
다른 모델에 비해 가지는 장점에 대해 논의해보자. 

## 3. Model Architecture

대부분의 뛰어난 신경망 기반 시퀀스 변환 모델들은 인코더-디코더 구조로 구성되어 있다. 
이 구조에서 인코더는 인풋 시퀀스 $x=(x_1, ... , x_n)$를 $z=(z_1, ..., z_n)$로 매핑한다. 
이때 $x$는 ("I","love","you")와 같은 기호 표현을 의미하고, 
$z$는 ((0.2, 0.3, -0.1),(0.5, -0.4, 0.2),(-0.1, 0.8, 0.3))과 같은 연속형 숫자 표현을 의미한다. 
그 후에 디코더는 $z$를 기반으로 출력 시퀀스 $(y_1, ..., y_m)$을 한번에 하나씩 생성한다. 
이때 디코더는 자기회귀(auto-regressive) 방식으로 작동하며, 
이전에 생성된 기호들을 다음 기호를 생성할 때 추가적인 입력으로 사용한다. 

(Figure 1. The Transformer - model architecture)

트랜스포머는 위 그림과 같은 전체적은 구조를 따른다. 
이 트랜스포머는 self-attention을 쌓은 형태와 
각각의 단어가 별도로 처리되는 완전 연결(full connected) 레이어를 
활용해 인코더와 디코더를 구성한다. 

이 구조는 figure1의 왼쪽과 오른쪽 그림에서 각각 볼 수 있다. 
왼쪽 그림은 인코더 구조, 오른쪽 그림은 디코더 구조를 보여준다. 
인코더와 디코더는 각각 self-attention 메커니즘을 통해 입력 시퀀스 간의 관계를 계산하고, 
이어지는 full connected layer를 통해 최종 출력을 생성한다. 

### 3.1. Encoder and Decoder Stacks



