---
title: "[딥러닝] Deep Q Network(DQN) 논문 번역" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---


# Deep Q Network(DQN) 논문 번역

[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

## 0. Abstract

우리는 강화학습을 통해서 고차원 센서로부터 직접 정책(policy)을 컨트롤함으로써 성공적으로 딥러닝 모델을 학습시켰다. 
여기서 말하는 딥러닝 모델은 CNN을 의미하는데 이는 Q learning으로 학습 시킨 것이다. 
이러한 모델을 학습 시킬 때 인풋은 각 픽셀(raw pixel)이 되고 아웃풋은 미래 보상에 대한 value function이다. 
우리는 이러한 방법으로 아타리 게임 2600개에 대한 학습 환경으로부터 학습 알고리즘이나 아키텍쳐 변화없이 학습시켰다. 

## 1.Introduction

비전이나 음성 데이터에서 고차원 센서 데이터로부터 나오는 데이터를 에이전트를 컨트롤함으로써 학습시키는 것은 
강화학습 분야에서 오랜 기간동안 도전적인 과제였습니다. 
이러한 도메인에서 가장 성공적인 강화학습 적용은 수작업으로 만든 피처를 linear value function이나 policy representations와 결합시키는 것이었습니다. 
당연하게도 이런 시스템에서의 학습 결과는 feature representation의 퀄리티에 의존적이었습니다. 

최근에와서는 raw 센서 데이터로부터 high-level 피처를 추출하는 것이 가능했고, 이는 컴퓨터 비전이나 음성 인식 분야에서 큰 발전을 이루어냈습니다. 
이러한 방법들은 CNN을 포함하는다양한 신경망 아키텍처를 활용했습니다. 
이 말은 기존에 사용했던 다양한 기술들을 강화학습에도 동일하게 사용할 수 있다는 뜻입니다. 

그러나 딥러닝적인 관점으로 보면 강화학습은 아직 챌린징한 부분이 많이 있습니다. 
첫번째 어려운 점은, 딥러닝을 학습시키기 위해서는 엄청나게 많은 수의 라벨링된 데이터가 필요합니다. 
반면 강화학습 알고리즘에서는 보상(reward)에 의해 학습되어야하는데 이때 보상이란 스칼라값을 의미합니다. 
강화학습에서 액션(action)과 보상(reward) 사이에는 딜레이가 존재하는데 이 딜레이는 매우 길어질수도 있어서 학습시 매우 큰 부담이 될 수 있습니다.  

두번째 어려운 점은 딥러닝 알고리즘에서는 데이터들의 독립성을 가정합니다. 
그러나 강화학습에서는 대부분 상태들이 높은 상관관계를 보여주는 시퀀스입니다. 
게다가 강화학습에서는 새로운 행동에 대해 데이터 분포가 바뀌어 버립는데, 이는 기반이 되는 분포를 가정하는 방법 자체를 사용하기 어렵게 만듭니다. 

이 논문은 CNN이 앞서 언급한 챌린징한 어려움들을 극복해서 성공적인 강화학습이 가능하다는 것을 보여줍니다. 
해당 신경망은 variant of Q-learning으로 학습하는데, 이는 stochastic gradient descent를 통해 가중치를 업데이트하는 방향으로 진행합니다. 
앞서 언급했던 문제점들 중 상관관계가 높은 데이터라는 점과 non-stationary 분포라는 문제점을 완화시키기 위해, 
experience replay mechanism을 사용할 것인데, 이는 previous transitons를 랜덤하게 샘플링함으로써 
많은 과거 행동들에 대한 학습 분포를 smooth하게 만듭니다. 

우리는 이러한 접근 방법으로 아케이드 학습 환경(ALE)에서 아타리 게임 2600개에 대해 적용할 것입니다. 
아타리 게임 2600의 데이터는 고차원 비주얼 인풋 데이터를 가지고 있는데 210x160 RGB video at 60Hz의 사양을 나타냅니다. 
그리고 각 게임은 인간 플레이어가 클리어하기 어려운 다양한 태스크로 구성되어 있습니다. 
우리의 목적은 가능한한 많은 게임을 성공적으로 학습하는 single neural network agent를 만드는 것입니다. 
해당 네트워크에게 게임에 대한 어떠한 정보를 제공하지 않으며, 수작업으로 만든 비주얼 피처같은 정보는 전혀 주어지지 않습니다. 
즉, 신경망은 인간의 플레이로부터 나오는 비디오 인풋, 보상, 최종 시그널, 가능한 행동으로부터만 학습됩니다. 
게다가 학습에 진행되는 네트워크 아키텍쳐나 모든 하이퍼파라미터는 게임 내내 상수로 취급됩니다. 
지금까지 네트워크는 기존의 전문가 인간 플레이어를 능가하는 성능을 보여줬습니다. 

## 2. Background

우리가 학습 대상으로 여기는 테스크는 한 에이전트는 환경 $\varepsilon$와 상호작용한다고 생각하겠습니다. 
이때 환경 $\varepsilon$이라는 것은 아타리 에뮬레이터, 액션 시퀀스(sequence of actions), 관측(observation), 보상(reward)로 구성됩니다. 
각 타임 스텝(time-step)에 대해 에이전트는 취할 수 있는 액션 $A=\\{ 1, \cdots, K \\}$ 중 하나인 액션 $a_t$를 선택합니다. 
해당 액션은 에뮬레이터를 통해 내부 상태(internal state)와 게임 스코어(game score)를 변동시킵니다. 
$\varepsilon$는 stochastic 합니다. 
에뮬레이터의 내부 상태는 에이전트에 의해 관측되지 않습니다. 
대신에, 에뮬레이터로부터 나오는 이미지 $x_t \in R^d$를 통해 관측할 수 있는데, 
이때 $x_t \in R^d$란 현재 스크린에 나타나는 로우 픽셀 벡터를 의미합니다. 
게다가 이후 보상 $r_t$를 받게 되는데 이는 게임 점수를 변화시킵니다. 
정리하면 게임 스코어는 이전의 액션 시퀀스(prior sequence of actions)와 관측(observation)에 의해 결정됩니다. 
어떤 액션에 대한 피드백은 수천 타임스텝이 지나고나서야 받을 수 있습니다.

에이전트는 오직 현재 스크린 이미지만 관측하기 때문에 전체의 부분적으로만 관측 가능하며 수많은 에뮬레이터 상태는 인식할수 없을 정도로 소외됩니다. 
즉, 연재 스크린 $x_t$만 가지고서는 현재 상황을 완벽하게 이해하는 것은 불가능합니다. 
그러므로 우리는 $s_t = x_1, a_1, x_2, ..., a_{t-1}, x_t$와 같은 액션 시퀀스와 관측치를 고려해서 게임 전략을 학습해야합니다. 
에뮬레이터에서의 모든 시퀀스는 유한한 타임 스텝에 대해 종료된다고 가정합니다. 
이러한 형식주의는 유한 Markov decision process(MDP)를 발전시킬 수 있는데, 이 때 MDP는 distinct state에 대한 시퀀스를 의미합니다. 
결과적으로 각 시점 $t$에 대해 완전한 시퀀스 $s_t$를 이용해 MDP에 대한 standard 강화학습 방법을 적용시킬 수 있습니다. 




