---
title: "[딥러닝] Deep Q Network(DQN) 논문 번역" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---


# Deep Q Network(DQN) 논문 번역

[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

## 0. Abstract

우리는 강화학습을 통해서 고차원 센서로부터 직접 정책(policy)을 컨트롤함으로써 성공적으로 딥러닝 모델을 학습시켰다. 
여기서 말하는 딥러닝 모델은 CNN을 의미하는데 이는 Q learning으로 학습 시킨 것이다. 
이러한 모델을 학습 시킬 때 인풋은 각 픽셀(raw pixel)이 되고 아웃풋은 미래 보상에 대한 value function이다. 
우리는 이러한 방법으로 아타리 게임 2600개에 대한 학습 환경으로부터 학습 알고리즘이나 아키텍쳐 변화없이 학습시켰다. 

## 1.Introduction

비전이나 음성 데이터에서 고차원 센서 데이터로부터 나오는 데이터를 에이전트를 컨트롤함으로써 학습시키는 것은 
강화학습 분야에서 오랜 기간동안 도전적인 과제였습니다. 
이러한 도메인에서 가장 성공적인 강화학습 적용은 수작업으로 만든 피처를 linear value function이나 policy representations와 결합시키는 것이었습니다. 
당연하게도 이런 시스템에서의 학습 결과는 feature representation의 퀄리티에 의존적이었습니다. 

최근에와서는 raw 센서 데이터로부터 high-level 피처를 추출하는 것이 가능했고, 이는 컴퓨터 비전이나 음성 인식 분야에서 큰 발전을 이루어냈습니다. 
이러한 방법들은 CNN을 포함하는다양한 신경망 아키텍처를 활용했습니다. 
이 말은 기존에 사용했던 다양한 기술들을 강화학습에도 동일하게 사용할 수 있다는 뜻입니다. 

그러나 딥러닝적인 관점으로 보면 강화학습은 아직 챌린징한 부분이 많이 있습니다. 
첫번째 어려운 점은, 딥러닝을 학습시키기 위해서는 엄청나게 많은 수의 라벨링된 데이터가 필요합니다. 
반면 강화학습 알고리즘에서는 보상(reward)에 의해 학습되어야하는데 이때 보상이란 스칼라값을 의미합니다. 
강화학습에서 액션(action)과 보상(reward) 사이에는 딜레이가 존재하는데 이 딜레이는 매우 길어질수도 있어서 학습시 매우 큰 부담이 될 수 있습니다.  

두번째 어려운 점은 딥러닝 알고리즘에서는 데이터들의 독립성을 가정합니다. 
그러나 강화학습에서는 대부분 상태들이 높은 상관관계를 보여주는 시퀀스입니다. 
게다가 강화학습에서는 새로운 행동에 대해 데이터 분포가 바뀌어 버립는데, 이는 기반이 되는 분포를 가정하는 방법 자체를 사용하기 어렵게 만듭니다. 

이 논문은 CNN이 앞서 언급한 챌린징한 어려움들을 극복해서 성공적인 강화학습이 가능하다는 것을 보여줍니다. 
해당 신경망은 variant of Q-learning으로 학습하는데, 이는 stochastic gradient descent를 통해 가중치를 업데이트하는 방향으로 진행합니다. 
앞서 언급했던 문제점들 중 상관관계가 높은 데이터라는 점과 non-stationary 분포라는 문제점을 완화시키기 위해, 
experience replay mechanism을 사용할 것인데, 이는 previous transitons를 랜덤하게 샘플링함으로써 
많은 과거 행동들에 대한 학습 분포를 smooth하게 만듭니다. 

우리는 이러한 접근 방법으로 아케이드 학습 환경(ALE)에서 아타리 게임 2600개에 대해 적용할 것입니다. 
아타리 게임 2600의 데이터는 고차원 비주얼 인풋 데이터를 가지고 있는데 210x160 RGB video at 60Hz의 사양을 나타냅니다. 
그리고 각 게임은 인간 플레이어가 클리어하기 어려운 다양한 태스크로 구성되어 있습니다. 
우리의 목적은 가능한한 많은 게임을 성공적으로 학습하는 single neural network agent를 만드는 것입니다. 
해당 네트워크에게 게임에 대한 어떠한 정보를 제공하지 않으며, 수작업으로 만든 비주얼 피처같은 정보는 전혀 주어지지 않습니다. 
즉, 신경망은 인간의 플레이로부터 나오는 비디오 인풋, 보상, 최종 시그널, 가능한 행동으로부터만 학습됩니다. 
게다가 학습에 진행되는 네트워크 아키텍쳐나 모든 하이퍼파라미터는 게임 내내 상수로 취급됩니다. 
지금까지 네트워크는 기존의 전문가 인간 플레이어를 능가하는 성능을 보여줬습니다. 

## 2. Background

우리가 학습 대상으로 여기는 테스크는 한 에이전트는 환경 $\varepsilon$와 상호작용한다고 생각하겠습니다. 
이때 환경 $\varepsilon$이라는 것은 아타리 에뮬레이터, 액션 시퀀스(sequence of actions), 관측(observation), 보상(reward)로 구성됩니다. 
각 타임 스텝(time-step)에 대해 에이전트는 취할 수 있는 액션 $A=\\{ 1, \cdots, K \\}$ 중 하나인 액션 $a_t$를 선택합니다. 
해당 액션은 에뮬레이터를 통해 내부 상태(internal state)와 게임 스코어(game score)를 변동시킵니다. 
$\varepsilon$는 stochastic 합니다. 
에뮬레이터의 내부 상태는 에이전트에 의해 관측되지 않습니다. 
대신에, 에뮬레이터로부터 나오는 이미지 $x_t \in R^d$를 통해 관측할 수 있는데, 
이때 $x_t \in R^d$란 현재 스크린에 나타나는 로우 픽셀 벡터를 의미합니다. 
게다가 이후 보상 $r_t$를 받게 되는데 이는 게임 점수를 변화시킵니다. 
정리하면 게임 스코어는 이전의 액션 시퀀스(prior sequence of actions)와 관측(observation)에 의해 결정됩니다. 
어떤 액션에 대한 피드백은 수천 타임스텝이 지나고나서야 받을 수 있습니다.

에이전트는 오직 현재 스크린 이미지만 관측하기 때문에 전체의 부분적으로만 관측 가능하며 수많은 에뮬레이터 상태는 인식할수 없을 정도로 소외됩니다. 
즉, 연재 스크린 $x_t$만 가지고서는 현재 상황을 완벽하게 이해하는 것은 불가능합니다. 
그러므로 우리는 $s_t = x_1, a_1, x_2, ..., a_{t-1}, x_t$와 같은 액션 시퀀스와 관측치를 고려해서 게임 전략을 학습해야합니다. 
에뮬레이터에서의 모든 시퀀스는 유한한 타임 스텝에 대해 종료된다고 가정합니다. 
이러한 형식주의는 유한 Markov decision process(MDP)를 발전시킬 수 있는데, 이 때 MDP는 distinct state에 대한 시퀀스를 의미합니다. 
결과적으로 각 시점 $t$에 대해 완전한 시퀀스 $s_t$를 이용해 MDP에 대한 standard 강화학습 방법을 적용시킬 수 있습니다. 

에이전트의 목표는 액션을 선택함으로써 미래 보상을 최대화 시키는 방향으로 에뮬레이터와 상호작용하는 것입니다. 
이때 미래 보상은 타임스텝 당 $\gamma$만큼 discount된다는 가정이 필요합니다. 
따라서 시점 $t$에 대해 discount된 미래 보상은 다음과 같이 정의할 수 있습니다.  

$R_t = \sum\limits_{t^\prime=t}^{T}\gamma^{t^{\prime} - t} r_{t^{\prime}}$   

위 식에서 $T$는 게임이 종료되기 까지 걸리는 전체 시간을 의미합니다. 
그리고 이제 최적화된 액션 value 함수(optimal action-value function) $Q^{*}(s,a)$를 정의하는데, 
이 함수는 maximum expected return을 달성하는 함수입니다. 즉, 기대 리턴값을 최대화 시키는 함수인 것입니다. 
이것이 가능하려면 어떤 액션-관측 시퀀스 $s$를 보고 어떤 전략을 선택함으로써 어떤 액션 $a$를 취하는 것으로 달성할 수 있습니다. 
이를 수식으로 나타내면 다음과 같습니다. 

$Q^*(s,a) = max_\pi E\[R_t \| s_t = s, a_t=a, \pi \]$   

위 식에서 $\pi$는 액션 시퀀스와 매핑되는 정책을 의미합니다. 

위와 같은 optimal action-value function은 Bellman equation이라는 규칙을 따릅니다. 
이것이 의미하는 바는 다음과 같습니다. 
만약 최적값 $Q^{\*}(s^{\prime}, a^{\prime})$가 주어진 시퀀스 $s^\prime$에서 
다음 타임 스텝에 대해 선택할 수 있는 모든 액션 $a^{\prime}$들 중 선택된 최적값이라면 
최적의 전략은 $r + \gamma Q^{\*}(s^\prime, a^\prime)$을 최대화 시키는 행동 $a^{\prime}$을 선택하는 것입니다. 
이를 수식으로 나타내면 다음과 같습니다.  

$Q^{\*}(s, a) = E_{s^\prime \sim \varepsilon}\[r + \gamma Q^{\*}(s^\prime, a^\prime) \| s,a\]$

수많은 강화학습 알고리즘에는 베이스 아이디어가 깔려있는데 그것은 Bellman equation을 업데이터 방식으로 채택함으로써 
$Q_{i+1}(s, a) = E_{s^\prime \sim \varepsilon}\[r + \gamma Q^{\*}(s^\prime, a^\prime) \| s,a \]$을 반복적으로 업데이트 함으로써 
action-value fucntion을 추정하는 것입니다. 
이러한 value iteration 알고리즘을 통해 optimal action-value function은 $i \rightarrow \infty$로 감에 따라 $Q_i \rightarrow Q^\*$로 수렴합니다. 
실제로는 이러한 접근 방식이 터무니 없다고 생각할 수 있는데 그 이유는 action-value function은 일반화 없이 각 시퀀스가 모두 따로따로 추정되기 떄문입니다. 
그 대신에 action-value function을 추정하는데 다음과 같은 근사치를 사용할 수 있습니다.  

$Q(s,a; \theta) \approx Q^{\*}(s,a)$  

강화학습 분야에서는 이 근사치를 일반적인 linear function approximator로 사용하는데, 
간혹가다가 딥러닝과 같은 non linear function approximator가 사용되기도 합니다. 
가중치 $\theta$를 적용한 Q-network로서 신경망 함수를 활용한 approximator를 사용할 수 있습니다. 
Q-network는 iteration $i$에 대해 다음과 같은 손실함수 $L_i(\theta_i)$ 시퀀스를 최소화 시키는 방향으로 학습할 수 있습니다. 

$L_i(\theta_i) = E_{s,a\sim \rho()}[(y_i - Q(s,a;\theta_i))^2]$  

위 식에서 $y_i = E_{s^\prime \sim \varepsilon}[r + \gamma max_{a^{\prime}}Q^{\*}(s^\prime, a^\prime) | \theta_{i-1}]$은 
$i$번째 iteration에 대한 타겟을 의미하며, $\rho(s,a)$는 시퀀스 $s$와 액션 $a$에 대한 확률 분포를 의미하는데 
우리는 이 분포를 behaviour distribution이라고 부르겠습니다. 
이전 iteration으로부터의 파라미터 $\theta_{i-1}$는 손실함수 $L_i(\theta_{i})$를 최적화 시키는 과정에서 고정되어 있습니다. 
이때 주의할 점은 타겟은 신경망의 가중치에 달려있다는 점에 주의해야합니다. 
이 사실은 타겟은 지도학습에 사용되기 전, 즉, 학습 시작전에 고정되어 있다는 기존 사실과 반대 됩니다. 
손실 함수를 가중치에 대해 미분하면 다음과 같은 그레디언트를 구할 수 있습니다. 

$\nabla_{\theta_i}L_{i}(\theta_i) = E_{s,a \sim \rho();s^{\prime}\sim \varepsilon}[(r+\gamma max_{a^\prime}Q(s^{\prime}, a^{\prime} ; \theta_{i-1})-Q(s,a; \theta_i))\nabla_{\theta_i}Q(s,a;\theta_i)]$

위와 같은 그레디언트에서 전체 기대값을 계산하기 보다는 stochastic gradient descent 방법을 활용해서 손실함수를 최적화 합니다. 
만약 가중치가 매 타임스텝 마다 업데이트 된다면 단 하나의 샘플에 대해 
behaviour distribution $\rho$와 에뮬레이터 $\varepsilon$각각에 대한 기대값이 변화합니다. 
그 후에 우리에게 익숙한 Q-learning 알고리즘에 다다를수있습니다. 

이 알고리즘은 모델에 대해 자유롭다는(model-free) 사실을 기억하세요. 
이것은 강화학습 테스크를 에뮬레이터 $\varepsilon$으로부터 나온 샘플을 통해 해결할 수 있는데, 
중요한 것은 명확한 $\varepsilon$에 대한 구성없이도 가능하다는 사실입니다. 
그리고 이것은 off-policy하다는 성질도 기억해주세요. 
이러한 방식의 학습은 각각의 state space의 적절한 탐험을 통해 behaviour distribution을 따르는 동안, 
$a = max_a Q(s,a; \theta)$에 대해 탐욕적 전략을 통해 학습합니다. 
실질적으로 behaviour distribution은 종종 $\epsilon$-greedy 전탁을 통해 선택되는데, 
이는 확률 $1-\epsilon$과 랜덤 액션을 선택할 확률 $\epsilon$을 통해 정해집니다. 

## 3. Related Work

강화학습 스토리중 가장 잘 알려진 것은 TD gammon일 것입니다. 
이것은 주사위 놀이를 하는 프로그램인데, 강화학습을 통한 셀프 플레이를 통해 슈퍼 휴먼 레벨 까지 도달했습니다. 
TD gammon을 학습할때는 Q learning과 비슷한 model-free 강화학습 알고리즘을 활용했습니다. 
그리고 value function을 근사시킬 때는 히든 레이어 1층 짜리 멀티 레이어 퍼셉트론을 활용했습니다. 
하지만 이 방법을 그대로 사용해서 체스에 적용했을 때는 성공적이지 않았습니다. 
이를 통해 TD gamma에 사용한 접근 방법이 매우 특이한 케이스라는 믿음이 널리 퍼졌습니다. 
왜냐하면 주사위를 굴릴때의 확률성이 state space를 탐험하는데 도움을 주고 
value function을 smooth 하게 만들어주는데 도움을 주었기 때문입니다. 

게다가 비선형 함수 근사를 통한 Q learning을 사용한 model-free 강화학습이나 
off-policy learning을 혼합하는 방법이 Q-network를 발산하게 만든다는 사실이 밝혀졌습니다. 
그 후에는 비선형이 아닌 선형 함수 근사를 활용한 강화학습이 수렴을 보장하는데 더 좋은 방법이라고 알려집니다. 

최근에는 다시 딥러닝과 강화학습을 혼합시키는 방법이 부활했습니다. 
딥러닝은 환경 $\varepsilon$을 추정하는데 사용되었고, 
Bolzmann 머신은 value function이나 정책을 추정하는데 사용되었습니다. 
게다가 Q learning이 발산하는 문제는 gradient temporal-difference methods로 부분적으로 다루어졌습니다. 
이러한 방법들은 비선형 함수 근사를 통한 fixed 정책을 통해 평가하거나 혹은 
제한된 Q learning variant를 활용한 선형 함수 근사를 활용한 control policy를 학습할때와 같은 상황에서 수렴한다고 입증이 되었습니다. 
그러나 이러한 방법들은 아직 비선형 컨트롤로 이어지지는 못했습니다. 

...(생략, 그 이후에도 열심히 연구했다는 내용)

## 4. Deep Reinforcement Learning

최근 딥러닝에서 컴퓨터 비전과 음성 인식 분야에서 엄청난 발전이 있었는데, 
그것은 아주 큰 트레이닝 데이터에 대해 효율적으로 학습을 이루어 낸 것입니다. 
로우 데이터에 대해 가장 성공적인 접근 방법은 stochastic gradient descent 방법을 기반으로한 
lightweight 업데이트 방식을 사용한 것이었습니다. 
딥러닝을 할 때 충분한 데이터를 제공함으로써 
수작업으로 직접만든 피처를 사용하는 것보다 더 나은 학습 결과를 보였습니다. 
이러한 성공은 강화학습에도 적용할 수 있겠다는 동기부여가 되었습니다. 
우리의 목적은 stochastic gradient updates를 활용해 RGB 이미지와 효율적으로 학습데이터를 처리하는 방식으로 작동하는 
강화학습과 딥러닝을 연결시키는 것입니다. 
