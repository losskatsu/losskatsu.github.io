---
title: "[최적화]Karush-Kuhn-Tucker(KKT)의 개념" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# Karush-Kuhn-Tucker(KKT)의 개념


### 참고링크  

|머신러닝|딥러닝|선형대수|기초통계|최적화|
|:------:|:------:|:------:|:------:|:------:|
|[k-means](https://losskatsu.github.io/machine-learning/kmeans-clustering/)|[신경망이란](https://losskatsu.github.io/machine-learning/dl-basic01/)|[고유값,고유벡터](https://losskatsu.github.io/linear-algebra/eigen/)| [확률변수](https://losskatsu.github.io/statistics/random-variable/) |[컨벡스 셋](https://losskatsu.github.io/machine-learning/convex-set/)|
|[k-최근접이웃](https://losskatsu.github.io/machine-learning/knn/)|[성능함수](https://losskatsu.github.io/machine-learning/dl-basic02/)|[행렬식](https://losskatsu.github.io/linear-algebra/determinant/)| [확률분포](https://losskatsu.github.io/statistics/prob-distribution/) | [컨벡스 함수](https://losskatsu.github.io/machine-learning/convex-function/)|
|[선형회귀](https://losskatsu.github.io/statistics/simple-regression/)|[신경망 학습](https://losskatsu.github.io/machine-learning/dl-basic03/)|[내적](https://losskatsu.github.io/linear-algebra/innerproduct/)| [모집단과 표본](https://losskatsu.github.io/statistics/population-sample/) |[라그랑주 듀얼](https://losskatsu.github.io/machine-learning/dual-function/)|
|[로지스틱회귀](https://losskatsu.github.io/statistics/logistic-regression/) |[교차연결](https://losskatsu.github.io/machine-learning/dl-basic04/) |[기저](https://losskatsu.github.io/linear-algebra/basis/)| [평균과 분산](https://losskatsu.github.io/statistics/mean-vairance/)   | [KKT 조건](https://losskatsu.github.io/machine-learning/kkt/) |
|[릿지,라쏘회귀](https://losskatsu.github.io/machine-learning/l1l2/) |[합성곱 신경망](https://losskatsu.github.io/machine-learning/dl-basic05/) |[랭크, 차원](https://losskatsu.github.io/linear-algebra/rank-dim/)| [공분산, 상관계수](https://losskatsu.github.io/statistics/cov-corr/)  | [ROC 커브](https://losskatsu.github.io/machine-learning/stat-roc-curve/) |
|[의사결정나무](https://losskatsu.github.io/machine-learning/decision-tree/) |[배치, 에포크 차이](https://losskatsu.github.io/machine-learning/epoch-batch/) | [선형변환](https://losskatsu.github.io/linear-algebra/linear-trans/)| [최대가능도추정](https://losskatsu.github.io/statistics/mle/) | [크로스 밸리데이션](https://losskatsu.github.io/machine-learning/cross-validation/) |
|[서포트벡터머신](https://losskatsu.github.io/machine-learning/svm/) | [텐서플로기초(1)](https://losskatsu.github.io/machine-learning/tensorflow-basic01/) |[직교행렬](https://losskatsu.github.io/linear-algebra/orthogonal/) | [베르누이,이항분포](https://losskatsu.github.io/statistics/binomial/)  | [실루엣 스코어](https://losskatsu.github.io/machine-learning/silhouette-score) |
|[원클래스 SVM](https://losskatsu.github.io/machine-learning/oneclass-svm/)  | [텐서플로기초(2)](https://losskatsu.github.io/machine-learning/tensorflow-basic02/)  | [고유값분해](https://losskatsu.github.io/linear-algebra/eigen-decomposition/)| [기하,음이항분포](https://losskatsu.github.io/statistics/geometric-negative/) | |
|[LDA ](https://losskatsu.github.io/machine-learning/lda/) | [seq2seq](https://losskatsu.github.io/machine-learning/seq2seq-keras/) | [특이값분해](https://losskatsu.github.io/linear-algebra/svd/) | [초기하분포](https://losskatsu.github.io/statistics/hypergeometric/) | |
|[GMM](https://losskatsu.github.io/machine-learning/gmm/) | [opencv기초](https://losskatsu.github.io/machine-learning/opencv01) | |[포아송분포](https://losskatsu.github.io/statistics/poisson/) | |
|[부스팅](https://losskatsu.github.io/machine-learning/boosting/) | [resnet](https://losskatsu.github.io/machine-learning/resnet) | | [정규분포](https://losskatsu.github.io/statistics/normaldist/) | |
|[사이킷런 실습](https://losskatsu.github.io/machine-learning/sklearn/) |[다각형내부판별](https://losskatsu.github.io/machine-learning/py-polygon01) | |[감마분포](https://losskatsu.github.io/statistics/gammadist/) | |
| | [엣지판별](https://losskatsu.github.io/machine-learning/edge-detect-canny) | | [지수분포](https://losskatsu.github.io/statistics/exponentialdist/) | |
| | | | [카이제곱분포](https://losskatsu.github.io/statistics/chisquareddist/) | |
| | | | [베타분포](https://losskatsu.github.io/statistics/betadist/) | |
| | | | [균일분포](https://losskatsu.github.io/statistics/uniformdist/) | |


<br/>

<a href="http://www.yes24.com/Product/Goods/97032765" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00001_ml.png" width="800" align="middle">

<br/>


## 컨벡스 문제(convex problem)에서의 KKT condition

프리멀 문제(primal problem)가 컨벡스(convex)일때, 
KKT 조건은 프리멀(primal), 듀얼(dual) 최적화 포인트임을 보이는데 충분하다. 
즉, 만약 $f_i$가 컨벡스이고, $h_i$가 아핀(affine)일 때, 
어떠한 지점(any point) $\tilde{x}, \tilde{\lambda}, \tilde{v}$에 대해서 아래와 같이 KKT 조건을 만족한다면, 
$\tilde{x}, (\tilde{\lambda}, \tilde{v})$는 프리멀, 듀얼 최적값(primal and dual optimal)이며, zero duality gap 입니다.  

$ f_i(\tilde{x}) \leq 0, \, i=1,\dots,m $  

$ h_i(\tilde{x})    = 0, \, i=1,\dots,p $   

$ \tilde{\lambda}_i \geq 0, \, i=1,\dots,m $   

$ \tilde{\lambda}_i f_i(\tilde{x}) = 0, \, i=1,\dots,m $   

$ \bigtriangledown f_0(\tilde{x}) + \sum_{i=1}^{m}\tilde{\lambda}_i \bigtriangledown f_i(\tilde{x}) + \sum_{i=1}^{p}\tilde{v}_i \bigtriangledown h_i(\tilde{x}) = 0 $   

처음 두가지 조건은 $\tilde{x}$가 primal feasible임을 보여줍니다. 
$\tilde{\lambda}_i \geq 0$ 이므로, $L(x, \tilde{\lambda}, \tilde{v})$ 는 $x$에 대해 컨벡스입니다. 
KKT에서 마지막 조건이 의미하는 바는, $x$에 대한 그래디언트(gradient)는 $x = \tilde{x}$에서 사라집니다. 
즉, $\tilde{x}$는 $L(x, \tilde{\lambda}, \tilde{v})$를 $x$에 대해 최소화시킵니다. 

$$
\begin{align}
g(\tilde{\lambda}) &= L(x, \tilde{\lambda}, \tilde{v}) \\
                   &= f_0(\tilde{x}) + \sum_{i=1}^{m}\tilde{\lambda}_i f_i(\tilde{x}) + \sum_{i=1}^{p}\tilde{v}_i h_i(\tilde{x}) \\
                   &= f_0(\tilde{x}) \\
\end{align}
$$  

마지막 부분에서는 $h_i(\tilde{x}) = 0$, $\tilde{\lambda}_i f_i(\tilde{x}) = 0$ 조건을 사용했습니다. 
이는 $\tilde{x}, (\tilde{\lambda}, \tilde{v})$ 는 zero duality gap을 가진다는 것을 보여주고, 
그러므로 primal and dual optimal 이 되겠습니다. 

정리하면, 컨벡스 최적화 문제에서 목적함수와 제약함수가 미분가능할때, 
KKT 조건을 만족하는 지점은 primal and dual 최적값이며, zero duality gap을 가집니다. 


<br/>

<a href="http://www.yes24.com/Product/Goods/105772247" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00002_la.png" width="800" align="middle">

<br/>
