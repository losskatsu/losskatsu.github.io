---
title: "[딥러닝] ResNet의 개념" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# 딥러닝 - ResNet의 개념

### 참고링크  

|머신러닝|딥러닝|선형대수|기초통계|최적화|
|:------:|:------:|:------:|:------:|:------:|
|[k-means](https://losskatsu.github.io/machine-learning/kmeans-clustering/)|[신경망이란](https://losskatsu.github.io/machine-learning/dl-basic01/)|[고유값,고유벡터](https://losskatsu.github.io/linear-algebra/eigen/)| [확률변수](https://losskatsu.github.io/statistics/random-variable/) |[컨벡스 셋](https://losskatsu.github.io/machine-learning/convex-set/)|
|[k-최근접이웃](https://losskatsu.github.io/machine-learning/knn/)|[성능함수](https://losskatsu.github.io/machine-learning/dl-basic02/)|[행렬식](https://losskatsu.github.io/linear-algebra/determinant/)| [확률분포](https://losskatsu.github.io/statistics/prob-distribution/) | [컨벡스 함수](https://losskatsu.github.io/machine-learning/convex-function/)|
|[선형회귀](https://losskatsu.github.io/statistics/simple-regression/)|[신경망 학습](https://losskatsu.github.io/machine-learning/dl-basic03/)|[내적](https://losskatsu.github.io/linear-algebra/innerproduct/)| [모집단과 표본](https://losskatsu.github.io/statistics/population-sample/) |[라그랑주 듀얼](https://losskatsu.github.io/machine-learning/dual-function/)|
|[로지스틱회귀](https://losskatsu.github.io/statistics/logistic-regression/) |[교차연결](https://losskatsu.github.io/machine-learning/dl-basic04/) |[기저](https://losskatsu.github.io/linear-algebra/basis/)| [평균과 분산](https://losskatsu.github.io/statistics/mean-vairance/)   | [KKT 조건](https://losskatsu.github.io/machine-learning/kkt/) |
|[릿지,라쏘회귀](https://losskatsu.github.io/machine-learning/l1l2/) |[합성곱 신경망](https://losskatsu.github.io/machine-learning/dl-basic05/) |[랭크, 차원](https://losskatsu.github.io/linear-algebra/rank-dim/)| [공분산, 상관계수](https://losskatsu.github.io/statistics/cov-corr/)  | [ROC 커브](https://losskatsu.github.io/machine-learning/stat-roc-curve/) |
|[의사결정나무](https://losskatsu.github.io/machine-learning/decision-tree/) |[배치, 에포크 차이](https://losskatsu.github.io/machine-learning/epoch-batch/) | [선형변환](https://losskatsu.github.io/linear-algebra/linear-trans/)| [최대가능도추정](https://losskatsu.github.io/statistics/mle/) | [크로스 밸리데이션](https://losskatsu.github.io/machine-learning/cross-validation/) |
|[서포트벡터머신](https://losskatsu.github.io/machine-learning/svm/) | [텐서플로기초(1)](https://losskatsu.github.io/machine-learning/tensorflow-basic01/) |[직교행렬](https://losskatsu.github.io/linear-algebra/orthogonal/) | [베르누이,이항분포](https://losskatsu.github.io/statistics/binomial/)  | [실루엣 스코어](https://losskatsu.github.io/machine-learning/silhouette-score) |
|[원클래스 SVM](https://losskatsu.github.io/machine-learning/oneclass-svm/)  | [텐서플로기초(2)](https://losskatsu.github.io/machine-learning/tensorflow-basic02/)  | [고유값분해](https://losskatsu.github.io/linear-algebra/eigen-decomposition/)| [기하,음이항분포](https://losskatsu.github.io/statistics/geometric-negative/) | |
|[LDA ](https://losskatsu.github.io/machine-learning/lda/) | [seq2seq](https://losskatsu.github.io/machine-learning/seq2seq-keras/) | [특이값분해](https://losskatsu.github.io/linear-algebra/svd/) | [초기하분포](https://losskatsu.github.io/statistics/hypergeometric/) | |
|[GMM](https://losskatsu.github.io/machine-learning/gmm/) | [opencv기초](https://losskatsu.github.io/machine-learning/opencv01) | |[포아송분포](https://losskatsu.github.io/statistics/poisson/) | |
|[부스팅](https://losskatsu.github.io/machine-learning/boosting/) | [resnet](https://losskatsu.github.io/machine-learning/resnet) | | [정규분포](https://losskatsu.github.io/statistics/normaldist/) | |
|[사이킷런 실습](https://losskatsu.github.io/machine-learning/sklearn/) |[다각형내부판별](https://losskatsu.github.io/machine-learning/py-polygon01) | |[감마분포](https://losskatsu.github.io/statistics/gammadist/) | |
| | [엣지판별](https://losskatsu.github.io/machine-learning/edge-detect-canny) | | [지수분포](https://losskatsu.github.io/statistics/exponentialdist/) | |
| | | | [카이제곱분포](https://losskatsu.github.io/statistics/chisquareddist/) | |
| | | | [베타분포](https://losskatsu.github.io/statistics/betadist/) | |
| | | | [균일분포](https://losskatsu.github.io/statistics/uniformdist/) | |


<br/>

<a href="http://www.yes24.com/Product/Goods/97032765" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00001_ml.png" width="800" align="middle">

<br/>


**설치 링크**

* [윈도우에 아나콘다를 사용해 파이썬 가상환경 설치하기](https://losskatsu.github.io/programming/py-conda/)
* [맥북에 파이썬 가상환경(pyenv) 설치하기](https://losskatsu.github.io/it-infra/pyenv-osx/)
* [우분투에 파이썬 가상환경(pyenv) 설치하기](https://losskatsu.github.io/programming/pyenv/)
* [CentOS에 파이썬 가상환경(pyenv) 설치하기](https://losskatsu.github.io/it-infra/pyenv-centos6/)
* [아나콘다 32비트 가상환경 설치하고, 키움 api 연동](https://losskatsu.github.io/it-infra/conda32/)


## 1. 기존 신경망의 문제점

인공지능 부야에서 뉴럴 네트워크 신경망은 정확도를 향상시키는데 큰 도움이 되었습니다. 
신경망에는 여러 종류가 있는데 이미지 처리를 할떄는 CNN(Convolutional Neural network)이 주로 사용되고, 
자연어 처리를 할때는 RNN(Recurrent Neural network)가 주로 사용됩니다. 
이들 신경망으로 학습을 시키면 훌륭한 결과를 보여주지만 단점이 전혀 없는 것은 아닙니다. 
그것은 바로 신경망의 레이어가 커지면 데이터를 학습시키기 어렵다는 단점이 있습니다.
그 이유는 신경망의 레이어가 깊어지면 vanishing gradient problem 때문에 error가 증가하기 때문입니다.  


## 2. vanishing gradient problem

오차역전파(backpropagation), 그래디언트(gradient)기반의 학습 방법은 모두 이런 문제를 가지고 있습니다. 
왜냐면 오차역전파나 그래디언트 방법을 쓰게 되면 신경망의 가중치(weight)를 업데이트하게 되는데, 
업데이트 하게 되는 과정에서 gradient 자체가 매우 작아지는 vanishing 문제가 생깁니다. 
이러한 문제를 해결하기위해 만들어진 것이 ResNet(Residual neural network) 입니다. 

## 3. ResNet

### 3.1. 입력값과 출력값의 차원이 동일한 경우

그렇다면 ResNet이란 무엇일까요? ResNet은 Residual neural network의 줄임말인데, 
이름을 보면 잔차(residual)과 관련이 있다는 것을 알 수 있습니다. 
ResNet을 사용하면 레이어의 인풋이 다른 레이어로 곧바로 건너 뛰어 버립니다. 
즉, 인풋 값들이 중간의 특정 레이어들을 모두 거치지 않고 한번에 건너 뛴다는 의미입니다. 
이를 그림으로 나타내면 다음과 같습니다.  

<center><img src="/assets/images/ml/resnet/resnet01.png" width="800"></center>

위 그림이 ResNet을 가장 잘 나타내는 그림입니다. 
그림을 잘보면 신경망의 입력값은 x인데 이 입력값 x가 가중치 레이어를 거쳐서 만들어지는 값이 
F(x)입니다. 그리고 오른쪽을보면 그냥 입력값 x자체가 레이어를 거치지 않고 그대로 건너 뛰는데 
이를 identity mapping이라고 합니다. identity mapping은 입력값과 출력값이 같은 매핑을 의미합니다. 
identity mapping은 skip connection 또는 identity shortcut connection이라고도 부릅니다.
따라서 x라는 입력값이 신경망을 거치고 난 결과는 가중치를 거친 값 F(x)와 identity mapping을 거쳐 만들어진 x 더한 
F(x)+x가 됩니다. 따라서 최종 출력값이 y라고 하면 y는 다음과 같이 쓸수 있습니다.

```python
y = F(x)+x
```

이것이 가장 간단한 형태의 ResNet입니다.

### 3.2. 입력값과 출력값이 동일하지 않은 경우 

위와 같은 경우는 입력값과 출력값이 동일한 차원일 때 해당하는 기본적인 ResNet 입니다. 
이번에는 가중치 레이어를 거친 결과와 identity mapping의 차원이 다를 경우를 생각해보겠습니다. 
예를 들어 CNN의 경우 입력값을 가중치 레이어에 넣고 합성곱을 하게 되면 입력 이미지 크기가 줄어들게 되는데 
이렇게 되면 줄어든 이미지 결과(F(x))와 기존 이미지 x의 행렬 크기가 달라서 덧셈을 할 수 없습니다. 
따라서 이런 경우에는 줄어든 행렬 F(x)에 맞추기 위해 identity mapping을 한 x에도 후처리를 해줘야합니다. 
이를 수식으로 나타내면 다음과 같ㅅ흡니다. 

```python
y = F(x,Wi)+Ws*x
```

위 식에서 Wi는 CNN 레이어에 속한 가중치이고, Ws는 identity mapping을 행렬 덧셈 가능하도록 곱해주는 기능을 한다고 보면 됩니다. 

<br/>

<a href="http://www.yes24.com/Product/Goods/105772247" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00002_la.png" width="800" align="middle">

<br/>
