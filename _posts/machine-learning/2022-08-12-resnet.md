---
title: "[딥러닝] ResNet의 개념" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# 딥러닝 - ResNet의 개념

**참고 링크**

* [윈도우에 아나콘다를 사용해 파이썬 가상환경 설치하기](https://losskatsu.github.io/programming/py-conda/)
* [맥북에 파이썬 가상환경(pyenv) 설치하기](https://losskatsu.github.io/it-infra/pyenv-osx/)
* [우분투에 파이썬 가상환경(pyenv) 설치하기](https://losskatsu.github.io/programming/pyenv/)
* [CentOS에 파이썬 가상환경(pyenv) 설치하기](https://losskatsu.github.io/it-infra/pyenv-centos6/)
* [아나콘다 32비트 가상환경 설치하고, 키움 api 연동](https://losskatsu.github.io/it-infra/conda32/)


## 1. 기존 신경망의 문제점

인공지능 부야에서 뉴럴 네트워크 신경망은 정확도를 향상시키는데 큰 도움이 되었습니다. 
신경망에는 여러 종류가 있는데 이미지 처리를 할떄는 CNN(Convolutional Neural network)이 주로 사용되고, 
자연어 처리를 할때는 RNN(Recurrent Neural network)가 주로 사용됩니다. 
이들 신경망으로 학습을 시키면 훌륭한 결과를 보여주지만 단점이 전혀 없는 것은 아닙니다. 
그것은 바로 신경망의 레이어가 커지면 데이터를 학습시키기 어렵다는 단점이 있습니다.
그 이유는 신경망의 레이어가 깊어지면 vanishing gradient problem 때문에 error가 증가하기 때문입니다.  


## 2. vanishing gradient problem

오차역전파(backpropagation), 그래디언트(gradient)기반의 학습 방법은 모두 이런 문제를 가지고 있습니다. 
왜냐면 오차역전파나 그래디언트 방법을 쓰게 되면 신경망의 가중치(weight)를 업데이트하게 되는데, 
업데이트 하게 되는 과정에서 gradient 자체가 매우 작아지는 vanishing 문제가 생깁니다. 
이러한 문제를 해결하기위해 만들어진 것이 ResNet(Residual neural network) 입니다. 

## 3. ResNet

그렇다면 ResNet이란 무엇일까요? ResNet은 Residual neural network의 줄임말인데, 
이름을 보면 잔차(residual)과 관련이 있다는 것을 알 수 있습니다. 
ResNet을 사용하면 레이어의 인풋이 다른 레이어로 곧바로 건너 뛰어 버립니다. 
즉, 인풋 값들이 중간의 특정 레이어들을 모두 거치지 않고 한번에 건너 뛴다는 의미입니다. 
이를 그림으로 나타내면 다음과 같습니다.  

-건너 뛰는 그림 넣을 예정-  



