---
title: "[머신러닝] 릿지(ridge), 라쏘(lasso) 회귀분석 개념" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# 릿지(ridge), 라쏘(lasso) 회귀분석 개념

### 참고링크  

|머신러닝|딥러닝|선형대수|기초통계|최적화|
|:------:|:------:|:------:|:------:|:------:|
|[k-means](https://losskatsu.github.io/machine-learning/kmeans-clustering/)|[신경망이란](https://losskatsu.github.io/machine-learning/dl-basic01/)|[고유값,고유벡터](https://losskatsu.github.io/linear-algebra/eigen/)| [확률변수](https://losskatsu.github.io/statistics/random-variable/) |[컨벡스 셋](https://losskatsu.github.io/machine-learning/convex-set/)|
|[k-최근접이웃](https://losskatsu.github.io/machine-learning/knn/)|[성능함수](https://losskatsu.github.io/machine-learning/dl-basic02/)|[행렬식](https://losskatsu.github.io/linear-algebra/determinant/)| [확률분포](https://losskatsu.github.io/statistics/prob-distribution/) | [컨벡스 함수](https://losskatsu.github.io/machine-learning/convex-function/)|
|[선형회귀](https://losskatsu.github.io/statistics/simple-regression/)|[신경망 학습](https://losskatsu.github.io/machine-learning/dl-basic03/)|[내적](https://losskatsu.github.io/linear-algebra/innerproduct/)| [모집단과 표본](https://losskatsu.github.io/statistics/population-sample/) |[라그랑주 듀얼](https://losskatsu.github.io/machine-learning/dual-function/)|
|[로지스틱회귀](https://losskatsu.github.io/statistics/logistic-regression/) |[교차연결](https://losskatsu.github.io/machine-learning/dl-basic04/) |[기저](https://losskatsu.github.io/linear-algebra/basis/)| [평균과 분산](https://losskatsu.github.io/statistics/mean-vairance/)   | [KKT 조건](https://losskatsu.github.io/machine-learning/kkt/) |
|[릿지,라쏘회귀](https://losskatsu.github.io/machine-learning/l1l2/) |[합성곱 신경망](https://losskatsu.github.io/machine-learning/dl-basic05/) |[랭크, 차원](https://losskatsu.github.io/linear-algebra/rank-dim/)| [공분산, 상관계수](https://losskatsu.github.io/statistics/cov-corr/)  | [ROC 커브](https://losskatsu.github.io/machine-learning/stat-roc-curve/) |
|[의사결정나무](https://losskatsu.github.io/machine-learning/decision-tree/) |[배치, 에포크 차이](https://losskatsu.github.io/machine-learning/epoch-batch/) | [선형변환](https://losskatsu.github.io/linear-algebra/linear-trans/)| [최대가능도추정](https://losskatsu.github.io/statistics/mle/) | [크로스 밸리데이션](https://losskatsu.github.io/machine-learning/cross-validation/) |
|[서포트벡터머신](https://losskatsu.github.io/machine-learning/svm/) | [텐서플로기초(1)](https://losskatsu.github.io/machine-learning/tensorflow-basic01/) |[직교행렬](https://losskatsu.github.io/linear-algebra/orthogonal/) | [베르누이,이항분포](https://losskatsu.github.io/statistics/binomial/)  | [실루엣 스코어](https://losskatsu.github.io/machine-learning/silhouette-score) |
|[원클래스 SVM](https://losskatsu.github.io/machine-learning/oneclass-svm/)  | [텐서플로기초(2)](https://losskatsu.github.io/machine-learning/tensorflow-basic02/)  | [고유값분해](https://losskatsu.github.io/linear-algebra/eigen-decomposition/)| [기하,음이항분포](https://losskatsu.github.io/statistics/geometric-negative/) | |
|[LDA ](https://losskatsu.github.io/machine-learning/lda/) | [seq2seq](https://losskatsu.github.io/machine-learning/seq2seq-keras/) | [특이값분해](https://losskatsu.github.io/linear-algebra/svd/) | [초기하분포](https://losskatsu.github.io/statistics/hypergeometric/) | |
|[GMM](https://losskatsu.github.io/machine-learning/gmm/) | [opencv기초](https://losskatsu.github.io/machine-learning/opencv01) | |[포아송분포](https://losskatsu.github.io/statistics/poisson/) | |
|[부스팅](https://losskatsu.github.io/machine-learning/boosting/) | [resnet](https://losskatsu.github.io/machine-learning/resnet) | | [정규분포](https://losskatsu.github.io/statistics/normaldist/) | |
|[사이킷런 실습](https://losskatsu.github.io/machine-learning/sklearn/) |[다각형내부판별](https://losskatsu.github.io/machine-learning/py-polygon01) | |[감마분포](https://losskatsu.github.io/statistics/gammadist/) | |
| | [엣지판별](https://losskatsu.github.io/machine-learning/edge-detect-canny) | | [지수분포](https://losskatsu.github.io/statistics/exponentialdist/) | |
| | | | [카이제곱분포](https://losskatsu.github.io/statistics/chisquareddist/) | |
| | | | [베타분포](https://losskatsu.github.io/statistics/betadist/) | |
| | | | [균일분포](https://losskatsu.github.io/statistics/uniformdist/) | |


<br/>

<a href="http://www.yes24.com/Product/Goods/97032765" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00001_ml.png" width="800" align="middle">

<br/>



## 회귀분석 기초 복습

우리가 추정하고자 하는 모델을 아래와 같다고 하겠습니다. 

$ y = f(z)+\epsilon, \, \, \, \epsilon ~(0, \sigma^2) $ 

그리고 우리가 만든 추정회귀함수를 아래와 같다고 하겠습니다. 

$ \hat{f(x)} = z^T\hat{\beta} $ 

또한 최소제곱법(least squared)으로 구한 최소제곱 추정 모수를 $\hat{\beta}^{ls}$라고 하겠습니다. 
이 때, 우리가 살펴봐야하는 점은 

1. $\hat{\beta}$이 실제 $\beta$에 충분히 가까운가?

2. $\hat{f(x)}$이 미래 관측값에 대해 잘 피팅(fit) 되는가?

1번에 대해 우리는 아래와 같이 MSE(mean squared error)를 사용합니다. 

$ MSE(\hat{\beta}) = E[\Vert \hat{\beta}-\beta \Vert^2] = E[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)] $ 

최소제곱법으로 구한 추정 모수에 대한 MSE를 구하면 아래와 같습니다.

$ E[(\hat{\beta}^{ls}-\beta)^T(\hat{\beta}^{ls}-\beta)] = \sigma^{2}tr[(Z^{T}Z)^{-1}] $ 

## 릿지(ridge) 회귀분석

제약이 없으면, $\beta_j$가 폭발(explode)적으로 커질수 있고, 이는 분산이 커지는 문제를 초래합니다. 
따라서 이러한 문제를 막기위해 제약식을 사용하는데, 릿지 회귀분석(ridge regression)은 아래와 같은 제약식을 사용합니다. 

$ minimize \sum_{i=1}^{n}(y_i - z_{i}^{T}\beta)^2 \,   \, \, \, s.t. \sum_{j=1}^{p}\beta_{j}^{2} \leq t $  

이는 아래와 같이 표현할 수도 있습니다. 

$ minimize (y-Z\beta)^T(y-Z\beta),  \, \, \, s.t. \sum_{j=1}^{p}\beta_{j}^{2} \leq t $ 

위에서, Z는 표준화(평균 0, 분산1)되었으며, y는 centered(평균값을 뺀) 되었습니다. 

<center><img src="/assets/images/ml/l1l2/01.jpg" width="800"></center>

이는 컨벡스이며, 따라서 unique한 해를 가집니다.

<center><img src="/assets/images/ml/l1l2/02.jpg" width="800"></center>


위 식에서 람다는 non-singular 문제로 만들어줍니다. 
심지어 $Z^T Z$가 invertible이 아닐지라도 말입니다. 

우리는 각각의 람다에 대해 해를 구할 수 있습니다. 
람다는 shrinkage parameter 이며, 

* $\lambda$는 계수(coefficient)의 사이즈를 조절합니다.
* $\lambda$는 정규식의 양을 조절합니다.
* $\lambda$가 0에 가까워질수록, 최소제곱추정량에 가까워집니다.
* $\lambda$가 무한대에 가까워질수록 $\hat{\beta}^{ridge}$는 0에 가까워집니다.(intercept-only model)

<center><img src="/assets/images/ml/l1l2/03.jpg" width="800"></center>



## 라쏘(lasso) 회귀분석

<center><img src="/assets/images/ml/l1l2/04.jpg" width="800"></center>

릿지와는 달리 라쏘추정량은 closed form이 없습니다. 
라소추정량을 구하려면 컨벡스 최적화에서 quadratic programming 기술이 필요합니다. 

**라쏘 추정 과정** 

1. 먼저 작은 $\epsilon$을 선택합니다. 

2. 시작은 잔차(residual) $r$을 $r=y$라고 설정하고 추정하고자하는 베타값들은 $\beta_1 =\beta_2 =\cdots=\beta_{p} = 0$이라고 설정합니다. 

3. 잔차 $r$과 가장 상관관계가 있는 피쳐 $Z_j(j=1,...,p)$를 선택합니다.

4. 이제 베타를 업데이트해야합니다. 새로운 $\beta_j$를 기존의 $\beta_j$에서 $\delta_j$를 더한값, 즉,  $\beta_j+\delta_j$로 업데이트합니다. 
이때  $\delta_j$란,  $\delta_j = \epsilon \cdot sign(r, Z_j) = \epsilon \cdot sign(Z_{j}^{T}r)$ 입니다.

5. 새로운 잔차 $r$을 업데이트합니다. 새로운 $r$를 $r-\delta_j Z_j$로 업데이트합니다.

6. 3단계~5단계를 반복합니다. 


<br/>

<a href="http://www.yes24.com/Product/Goods/105772247" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00002_la.png" width="800" align="middle">

<br/>

