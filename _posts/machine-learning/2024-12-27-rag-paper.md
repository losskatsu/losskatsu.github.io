---
title: "[딥러닝] RAG(Retrieval Augmented Generation) 논문 번역" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---


# RAG(Retrieval Augmented Generation) 논문 번역

## 0. Abstract

사전 학습된 대규모 언어 모델(large language model)은 파리미터에서 사실적 지식을 저장할 수 있으며, 
다운스트림(downstream) NLP 작업에서 최첨단 성능을 달성하는 것으로 나타났다. 
그러나 이러한 모델이 지식에 접근하고 정밀하게 조작하는 능력은 여전히 제한적이라서 
지식 집약적 작업에서는 과제 특화적인 아키텍처에 비해 성능이 뒤쳐진다. 
또한 의사결정에 대한 출처를 제공하고, 
그들 세계의 지식을 업데이트 하는 것은 여전히 해결되지 않은 연구 과제로 남아있다. 
명시적인(explicit) nonparametric 메모리에 대해 식별 가능한 접근 메커니즘으로 무장된 사전 학습 모델은 
이 문제를 극복할 수 있지만, 
현재까지는 추출 기반 다운스트림 작업에 대해서만 연구되었다. 
이때 nonparametric이라는 단어의 뜻을 알아보자. 
일반적인 언어모델은 학습 과정에서 지식을 모델의 파라미터에 암묵적으로 저장한다. 
즉, 신경망 가중치 자체에 정보가 내재한다는 뜻이다. 
반면 nonparametric 메모리는 외부 데이터 저장소처럼 작동하며, 학습된 가중치와는 별개로 유지된다. 
또한 명시적(explicit)이라는 말은 메모리에 저장된 데이터가 구조적으로 정리되어 있으며, 
명확하게 접근가능함을 의미한다. 
이는 모델이 필요한 정보를 명확히 요청하고 검색할 수 있다는 것을 내포한다. 
우리는 RAG(Retrieval Augmented Generation)를 위한 범용 파인튜닝 방식을 알아볼 것이다. 
이때 RAG 모델은 사전 학습된 파라미터 메모리와 nonparameter 메모리를 결합하여 언어 생성을 수행한다.
우리는 이 논문에서 RAG 모델을 소개하는데, parametric 메모리는 사전 학습된 seq2seq 모델이고, 
nonparametric 메모리는 사전학습된 신경망 검색기(neural retriever)를 통해  접근하는 
위키피디아의 밀집 벡터 인덱스(dense vector index)이다. 
여기서 검색기(neural retriever)란 검색 쿼리에 대해 적절한 문서를 찾아오는 역할을 담당하는 
데이터 검색의 핵심 컴포넌트를 의미한다. 
검색기는 검색 쿼리를 해석하고 인덱스 데이터를 활용해 데이터를 찾고, 필터링을 한 후 
쿼리와 가장 관련성이 높은 데이터만 선발해 리턴한다. 
우리는 두가지 RAG 구성 방식을 비교할 것이다. 
첫번째는 생성된 전체 시퀀스에 대해 동일하게 검색된 문서를 조건으로 사용하는 방식이고, 
두번째는 토큰마다 다른 문서를 사용할 수 있는 방식이다. 
우리는 다양한 지식 집약적인 NLP 작업에서 모델을 파인튜닝하고 평가했으며, 
세가지 오픈 도메인 QA 작어벵서 새로운 최고 성능을 달성했다. 
이는 파라미터 기반 seq2seq 모델과 과제에 특화된 검색-추출 아키텍처를 능가하는 결과이다. 
언어 생성 작업에서, RAG 모델이 최첨단 파라미터 전용 seq2seq 기준 모델에 비해 더 구체적이고 
다양하며, 사실적인 언어를 생성한다는 것을 발견했다. 

<br/>

<a href="http://www.yes24.com/Product/Goods/97032765" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00001_ml.png" width="800" align="middle">

<br/>

## 1. Introduction

사전 학습된 신경망 언어 모델은 데이터로부터 상당히 깊이 있는 지식을 학습할 수 있다는 사실이 입증되었다. 
이들은 외부 메모리에 접근하지 않고도 매개변수화된(parameterized) 암묵적 지식 베이스로서 이를 수행할 수 있다. 
이런 발전은 흥미롭지만, 이와 같은 모델에는 몇가지 단점이 존재한다. 
이들은 그들의 메모리를 쉽게 확장하거나 수정할 수 없으며, 
그들이 한 예측에 대해 명확한 통찰을 제공하기 어렵고, 
때로는 할루시네이션과 같은 잘못된 정보를 생성할 수도 있다. 
파라미터 매모리와 non-parametric(즉, 검색기반) 메모리를 결합한 하이브리드 모델은 
이러한 문제 중 일부를 해결할 수 있다. 
이러한 모델은 지식을 직접 수정하고 확장할 수 있으며, 
접근한 지식을 검사하고 해석할 수 있기 때문이다. 
REALM과 ORQA는 최근 도입된 모델로서, 마스킹된 언어 모델과 미분 가능한 검색기(differentiable retriever)를 결합한 방식을 사용한다. 
이들은 오직 오픈 도메인 추출 질문 응답에만 초점이 맞춰져있다. 
이때 미분 가능한 검색기(differentiable retriever)에 대해 알아보자. 
키워드 매칭이나 규칙 기반으로 검색하는 과거의 전통적인 검색기와는 달리 
미분 가능한 검색기(differentiable retriever)는 검색 과정 자체가 신경망으로 구성되어 있어, 
딥러닝 모델처럼 학습 가능하다. 
이들은 검색 결과를 모델의 손실 함수에 연결하여, 검색기의 파라미털플 미분 및 업데이트 할 수 있다. 

(Figure 1.)

여기에서 우리는 NLP의 주력 모델인 seq2seq 모델에 하이브리드 파라미터 메모리와 
non-parametric 메모리를 도입한다. 

우리는 사전 학습된 파라미터 메모리 생성 모델에 non-parametric 메모리를 추가하여, 
검색 증강 생성(Retrieval-Augmented Generation, RAG)라고 부르는 
범용(general-purpose) 파인튜닝 방식을 통해 이를 구현한다. 
우리는 RAG 모델을 구축했다. 
우리가 구축한 RAG 모델에서 파라미터 메모리는 사전 학습된 seq2seq 트랜스포머이고, 
non-parametric 메모리는 사전 학습된 신경망 검색기를 통해 접근하는 
위키핃디아의 밀집 벡터 인덱스(dense vector index)이다.
우리는 이러한 구성 요소를 확률적 모델로 결합하여 엔드투엔드(end-to-end) 방식으로 학습한다. 
이는 figure1에서 확인할 수 있다. 
검색기(Dense Passage Retriever, 이하 DPR)는 인풋에 따라 잠재 문서를 제공한다. 
이후 seq2seeq 모델(BART)은 이 잠재 문서와 입력을 함께 조건으로 사용하여 출력을 생성한다. 
우리는 잠재 문서를 통해 상위-K 근사를 사용해 주변화(marginalize)한다. 
이는 출력 단위로(모든 토큰에 대해 동일한 문서가 책임진다고 가정) 수행하거나, 
토큰 단위로(각 토큰에 대해 다른 문서가 책임진다고 가정) 수행할 수 있다. 
T5나 BART와 마찬가지로, RAG는 모든 seq2seq 작업에 대해 파인튜닝이 가능하며, 
이 과정에서 생성기와 검색기가 공동으로 학습된다. 


<br/>

<a href="http://www.yes24.com/Product/Goods/106175772" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00003_crawling.png" width="800" align="middle">

<br/>




<br/>

<a href="http://www.yes24.com/Product/Goods/105772247" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00002_la.png" width="800" align="middle">

<br/>





<br/>

<a href="http://www.yes24.com/Product/Goods/117709828" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00004_probability.png" width="800" align="middle">

<br/>





<br/>

<a href="http://www.yes24.com/Product/Goods/126115324" target="_blank"><img src="/assets/images/advertisement/ad-book/ad00005_dockernk8s.png" width="800" align="middle">

<br/>

