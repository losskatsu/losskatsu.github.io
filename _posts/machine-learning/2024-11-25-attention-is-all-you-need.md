---
title: "[딥러닝] Attention is all you need 논문 번역" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---


# Attention is all you need 논문 번역

## 0. Abstract

대부분의 시퀀스 변환 모델들은 복잡한 순환신경망(RNN)이나 합성곱신경망(CNN)을 기반으로 하는 경우가 많았고, 
이러한 신경망들은 인코더(encoder)와 디코더(decoder)를 포함한다. 
또한 성능이 가장 뛰어난 모델들은 어텐션 메커니즘을 통해 인코더와 디코더를 연결한다. 
우리는 이 논문에서 트랜스포머(Transformer)라는 아주 단순한 신경망 구조를 제안한다. 
이 트랜스포머라는 구조는 RNN이나 CNN은 제거하고 단순히 어텐션 메커니즘을 기반으로 하는 아주 간단한 구조이다. 
우리는 번역 업무를 수행하는 두 종류의 인공지능으로 실험을 했는데, 
트랜스포머 기반의 모델이 품질이 더 우수했으며 병렬처리 하기 좋고, 학습하는 시간도 적게들었다. 
우리 모델은 WMT 2014 영어-독일어 번역 업무에서 28.4 BLEU를 기록했는데, 
이는 기존 기록보다 2 BLEU 이상 좋은 성능을 보여주면서 기존 베스트 기록을 갈아치웠다. 
그리고 WMT 2014 영어-프랑스 번역 업무에서는 단일 모델 기준, 
41.8 BLEU라는 초대 신기록을 달성했는데, 
이는 3.5일 동안 8개의 GPU로 학습한 결과인데, 
이는 흔히 말하는 베스트 모델에 비해 현저히 적은 리소스로 학습한 것을 의미합니다. 
트랜스포머 구조는 대규모 학습 데이터냐 제한된 학습 데이터냐에 상관없이 
영어 구문 분석에서 성공적으로 적용할 수 있음을 보여주었다. 

## 1. Introduction

RNN, LSTM, GRU와 같은 모델들은 언어 모델이나 기계번역과 같은 시퀀스 모델링이나 변환 문제에서 대세로 자리잡았습니다. 
그리고 인코더-디코더 아키텍트를 발전시키기 위한 수많은 사람들의 노력이 있었다. 

순환 모델들은 일바적으로 인풋이나 아웃풋 시퀀스의 기호 위치를 따라 팩터를 계산한다. 
시퀀스의 기호 위치는 계산 과정에서 특정 스텝에 할당딘 후, 
은닉 상태 $h_t$를 생성합니다. 
이때 은닉 상태 $h_t$는 이전 은닉상태 $h_{t-1}$과 현재 시점 $t$에서의 인풋값의 함수이다. 
그러나 이러한 방식으로는 학습과정에서 데이터의 병렬 처리를 어렵게 만들고 시퀀스 길이가 길어질수록 더 큰 문제가 된다.  
왜냐하면 메모리 제약조건이 데이터간 배치를 제한하기 때문이다. 
최근 연구에서는 팩토라이제이션 트릭과 조건부 계산을 활용해 계산 효율성을 높이고, 
조건부 계산의 경우에는 모델 성능도 끌어올렸다. 
그러나 순차적으로 계산된다는 근본적인 제약조건은 여전히 남아있는 상태였다.  
